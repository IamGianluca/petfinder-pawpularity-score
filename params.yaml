create_folds:
  seed: 42
  k: [5, 10]

# resize:
#   sz: [224, 384]

# resize_extra_images:
#   sz: [224, 384]

train_one:
  name: one
  seed: 5899
  n_folds: 5
  fold: -1
  # problem
  metric: rmse
  metric_mode: min
  # input images
  train_data: data/train
  # model
  arch: swin_large_patch4_window12_384
  pretrained: true
  epochs: 6
  bs: 16
  auto_batch_size: false
  accumulate_grad_batches: 1
  precision: 16
  # augmentations
  use_normalize: true
  n_tfms: 2
  magn: 3
  sz: 384
  use_mix: 0
  mix_p: 0.0
  resize: -1
  # regularization
  dropout: 0.0
  wd: 0.0
  label_smoothing: 0.1
  # optimizer
  loss: bce_with_logits
  opt: adamw
  sched: cosine
  lr: 0.000006
  warmup_epochs: 1
  auto_lr: false
  mom: 0.9

train_two:
  name: two
  seed: 7591
  n_folds: 5
  fold: -1
  # problem
  metric: rmse
  metric_mode: min
  # input images
  train_data: data/train
  # model
  arch: swin_large_patch4_window7_224
  pretrained: true
  epochs: 6
  bs: 64
  auto_batch_size: false
  accumulate_grad_batches: 1
  precision: bf16
  # augmentations
  use_normalize: true
  n_tfms: 1
  magn: 5
  sz: 224
  use_mix: 0
  mix_p: 0.0
  resize: -1
  # regularization
  dropout: 0.0
  wd: 0.0
  label_smoothing: 0.1
  # optimizer
  loss: bce_with_logits
  opt: adamw
  sched: cosine
  warmup_epochs: 1
  lr: 0.000024
  auto_lr: false
  mom: 0.9

train_three:
  name: three
  seed: 9102
  n_folds: 5
  fold: -1
  # problem
  metric: rmse
  metric_mode: min
  # input images
  train_data: data/train
  # model
  arch: xcit_large_24_p8_224_dist
  pretrained: true
  epochs: 6
  bs: 16
  auto_batch_size: false
  accumulate_grad_batches: 1
  precision: bf16
  # augmentations
  use_normalize: true
  n_tfms: 1
  magn: 5
  sz: 224
  use_mix: 0
  mix_p: 0.0
  resize: -1
  # regularization
  dropout: 0.0
  wd: 0.0
  label_smoothing: 0.1
  # optimizer
  loss: bce_with_logits
  opt: adamw
  sched: cosine
  warmup_epochs: 1
  lr: 0.00001
  auto_lr: false
  mom: 0.9

train_four:
  name: four
  seed: 1230
  n_folds: 5
  fold: -1
  # problem
  metric: rmse
  metric_mode: min
  # input images
  train_data: data/train
  # model
  arch: cait_s24_224
  pretrained: true
  epochs: 6
  bs: 64
  auto_batch_size: false
  accumulate_grad_batches: 1
  precision: bf16
  # augmentations
  use_normalize: true
  n_tfms: 1
  magn: 5
  sz: 224
  use_mix: 0
  mix_p: 0.0
  resize: -1
  # regularization
  dropout: 0.0
  wd: 0.0
  label_smoothing: 0.1
  # optimizer
  loss: bce_with_logits
  opt: adamw
  sched: cosine
  warmup_epochs: 1
  lr: 0.00004
  auto_lr: false
  mom: 0.9

ensemble:
  name: ensemble
  seed: 1616
  models: [one, two, three, four]
  n_folds: 5

pseudo_labeling:
  models: [one, two, three, four]
  n_folds: 5

train_one_extra2:
  name: one_extra2
  seed: 6921
  n_folds: 5
  fold: -1
  # problem
  metric: rmse
  metric_mode: min
  # input images
  train_data: data/train
  use_extra_images: 2
  # model
  arch: swin_large_patch4_window12_384
  pretrained: true
  epochs: 6
  bs: 16
  auto_batch_size: false
  accumulate_grad_batches: 1
  precision: bf16
  # augmentations
  use_normalize: true
  n_tfms: 2
  magn: 3
  sz: 384
  use_mix: 0
  mix_p: 0.0
  resize: -1
  # regularization
  dropout: 0.0
  wd: 0.0
  label_smoothing: 0.1
  # optimizer
  loss: bce_with_logits
  opt: adamw
  sched: cosine
  lr: 0.000006
  warmup_epochs: 1
  auto_lr: false
  mom: 0.9

train_two_extra2:
  name: two_extra2
  seed: 7591
  n_folds: 5
  fold: -1
  # problem
  metric: rmse
  metric_mode: min
  # input images
  train_data: data/train
  use_extra_images: 2
  # model
  arch: swin_large_patch4_window7_224
  pretrained: true
  epochs: 6
  bs: 64
  auto_batch_size: false
  accumulate_grad_batches: 1
  precision: bf16
  # augmentations
  use_normalize: true
  n_tfms: 1
  magn: 5
  sz: 224
  use_mix: 0
  mix_p: 0.0
  resize: -1
  # regularization
  dropout: 0.0
  wd: 0.0
  label_smoothing: 0.1
  # optimizer
  loss: bce_with_logits
  opt: adamw
  sched: cosine
  warmup_epochs: 1
  lr: 0.00002
  auto_lr: false
  mom: 0.9

train_three_extra2:
  name: three_extra2
  seed: 9102
  n_folds: 5
  fold: -1
  # problem
  metric: rmse
  metric_mode: min
  # input images
  train_data: data/train
  use_extra_images: 2
  # model
  arch: xcit_large_24_p8_224_dist
  pretrained: true
  epochs: 6
  bs: 16
  auto_batch_size: false
  accumulate_grad_batches: 1
  precision: bf16
  # augmentations
  use_normalize: true
  n_tfms: 1
  magn: 5
  sz: 224
  use_mix: 0
  mix_p: 0.0
  resize: -1
  # regularization
  dropout: 0.0
  wd: 0.0
  label_smoothing: 0.1
  # optimizer
  loss: bce_with_logits
  opt: adamw
  sched: cosine
  warmup_epochs: 1
  lr: 0.00001
  auto_lr: false
  mom: 0.9

train_four_extra2:
  name: four_extra2
  seed: 1230
  n_folds: 5
  fold: -1
  # problem
  metric: rmse
  metric_mode: min
  # input images
  train_data: data/train
  use_extra_images: 2
  # model
  arch: cait_s24_224
  pretrained: true
  epochs: 6
  bs: 128
  auto_batch_size: false
  accumulate_grad_batches: 1
  precision: bf16
  # augmentations
  use_normalize: true
  n_tfms: 1
  magn: 5
  sz: 224
  use_mix: 0
  mix_p: 0.0
  resize: -1
  # regularization
  dropout: 0.0
  wd: 0.0
  label_smoothing: 0.1
  # optimizer
  loss: bce_with_logits
  opt: adamw
  sched: cosine
  warmup_epochs: 1
  lr: 0.00004
  auto_lr: false
  mom: 0.9

ensemble_final:
  name: ensemble_final
  seed: 1010
  n_folds: 5
  models: [one_extra2, two_extra2, three_extra2, four_extra2]
